{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRNet_models_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1CfZr__riY1RwLWtPx4FhIhmCHGfX1rVn",
      "authorship_tag": "ABX9TyMEHOJLp53IhRPCOMtDDW98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamaleldin/MRNET-for-Knee-Diagnosis/blob/master/MRNet_models_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwQJbS4DNac",
        "colab_type": "text"
      },
      "source": [
        "## **All needed library imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQvR4fkNDRyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "988bc8b8-0ab8-4b0d-bbe2-ae5734765bd5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqvYMolL1w_D",
        "colab_type": "text"
      },
      "source": [
        "# **All needed labels and constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnCBigQXyCjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we define the labels to be used in obtaining the data\n",
        "# series\n",
        "axial = 'axial';\n",
        "coronal = 'coronal';\n",
        "sagittal = 'sagittal';\n",
        "# data set\n",
        "valid = 'valid';\n",
        "train = 'train';\n",
        "# symptom\n",
        "acl = 'acl';\n",
        "meniscal = 'meniscus';\n",
        "abnormal = 'abnormal';\n",
        "# models\n",
        "vgg = 'VGG16';\n",
        "inception = 'Inception V3';\n",
        "resnet = 'Resnet';\n",
        "# model types\n",
        "extractor = 'Extractor';\n",
        "classifier = 'Classifier';\n",
        "regressor = 'Regressor';\n",
        "# paths\n",
        "path_data = '/content/drive/My Drive/MRNET data set/MRNet-v1.0';\n",
        "path_model = '/content/drive/My Drive/Models';\n",
        "delim = '/';\n",
        "# extensions\n",
        "extension_model = '.h5';\n",
        "extension_numpy = '.npy';\n",
        "extension_csv = '.csv';\n",
        "# total data\n",
        "total_train = 1130;\n",
        "total_test = 120;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ITqvN9Pxq1E",
        "colab_type": "text"
      },
      "source": [
        "# **Obtaining the data for training in a correct format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDisfeedicdn",
        "colab_type": "code",
        "outputId": "8991914e-baca-459b-c07c-64073d1f1975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# importing the drive to obtain the data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW7xpE0nFUH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipping the data zip file run once\n",
        "!unzip '/content/drive/My Drive/MRNET data set/MRNet-v1.0.zip' -d '/content/drive/My Drive/MRNET data set'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_sk_qq714vT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to obtain the data from drive to memory for training\n",
        "def get_data_train(series,anomaly,start = 0,batch_size = total_train):\n",
        "  input_data = [];\n",
        "  for exam_no in range(start,batch_size):\n",
        "      path_input = path_data + delim  + train + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n",
        "      input_data.append(np.stack([np.load(path_input)] * 3, axis = 3))\n",
        "  path_output = path_data + delim + train + '-' + anomaly + extension_csv;\n",
        "  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n",
        "  return input_data, output_data[start:batch_size];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e5s8drLFYwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to obtain the data from drive to memory for testing\n",
        "def get_data_test(series,anomaly,start = 0,batch_size = total_test):\n",
        "  input_data = [];\n",
        "  for iterator in range(start,batch_size):\n",
        "      exam_no = iterator + total_train;\n",
        "      path_input = path_data + delim  + valid + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n",
        "      input_data.append(np.stack([np.load(path_input)] * 3, axis = 3))\n",
        "  path_output = path_data + delim + valid + '-' + anomaly + extension_csv;\n",
        "  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n",
        "  return input_data, output_data[start:batch_size];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByiaFQDs13YQ",
        "colab_type": "text"
      },
      "source": [
        "# **Data normalization part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJA7UyZs18MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalizing the image pixels to be a number from 0 to 1\n",
        "def normalize(X):\n",
        "  for x in X:\n",
        "    x = x/255\n",
        "  return X;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17OHcLkqYLQY",
        "colab_type": "text"
      },
      "source": [
        "# **Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQU_i1J8YQcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on augmentation techniques defined in the paper\n",
        "def get_generators():\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rotation_range=25,\n",
        "      width_shift_range=[-25,25],\n",
        "      height_shift_range=[-25,25],\n",
        "      horizontal_flip=True);\n",
        "  validation_datagen = ImageDataGenerator();\n",
        "  return train_datagen,validation_datagen;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsEVz5V8CB-K",
        "colab_type": "text"
      },
      "source": [
        "# **Data splitting part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6mSwzflCHLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting the data to train and validate with ratios 90% to 10%\n",
        "def split(X , Y):\n",
        "  return X[:3],Y[:3],X[3:],Y[3:];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4uMa5xI90O",
        "colab_type": "text"
      },
      "source": [
        "# **Training functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67C5BfqJJCgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_extractor(model:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # processing to train for the extractor specifically: obtaining only one image from each patient\n",
        "  input_train = get_one_image(input_train);\n",
        "  input_validate = get_one_image(input_validate);\n",
        "  # normalizing the input data\n",
        "  input_train = normalize(input_train);\n",
        "  # performing data augmentation\n",
        "  train_datagen,validation_datagen = get_generators();\n",
        "  train_generator = train_datagen.flow(input_train,output_train, batch_size = 20);\n",
        "  validation_generator = validation_datagen.flow(input_validate,output_validate, batch_size = 20);\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + extractor + delim+series + '_' + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = model.fit_generator(train_generator, validation_data=validation_generator, epochs=50, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwhjnY5nN2hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_image(input_train):\n",
        "  new_input_train = [];\n",
        "  for curr in input_train:\n",
        "     new_input_train.append(curr[0]);\n",
        "  return np.array(new_input_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvHgIA-tjsSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(extractor:Model,binary_classifier:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # normalizing the input data\n",
        "  input_train = normalize(input_train);\n",
        "  # obtaining the data specifically for the classifier by getting the extracted features\n",
        "  input_train = get_features(extractor,input_train);\n",
        "  input_validate = get_features(extractor,input_validate);\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + classifier + delim + series + '_' + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = binary_classifier.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co61bPJokios",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(extractor:Model,input):\n",
        "  # forming the extraction model\n",
        "  model = keras.models.Sequential();\n",
        "  model.add(extractor);\n",
        "  model.add(keras.layers.GlobalAveragePooling2D(data_format='channels_last'));\n",
        "  # extracting the features\n",
        "  result = [];\n",
        "  for curr in input:\n",
        "    # getting the predictions\n",
        "    features_extracted = model.predict(curr);\n",
        "    # getting maximum between the batches\n",
        "    max_features = np.max(features_extracted,axis=0);  \n",
        "    result.append(max_features);\n",
        "  return np.array(result);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtlVeEI450Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_regressor(regressor:Model,extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n",
        "  # predictions of all classifiers:\n",
        "  pred_axial,output_train,val_axial,output_validate = get_classifications(extractor_axial,model_type,axial,anomaly);\n",
        "  pred_sagittal,output_train,val_sagittal,output_validate = get_classifications(extractor_sagittal,model_type,sagittal,anomaly);\n",
        "  pred_coronal,output_train,val_coronal,output_validate = get_classifications(extractor_coronal,model_type,coronal,anomaly);\n",
        "  # combining the predictions\n",
        "  input_train = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n",
        "  input_validate = np.array([val_axial,val_coronal,val_sagittal]).reshape(3,val_axial.shape[0]).T;\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + regressor + delim + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = binary_classifier.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvjxrjGg6FMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classifications(extractor:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly,start = 0,batch_size = 5);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # normalizing the input data\n",
        "  input_train = normalize(input_train);\n",
        "  # obtaining the features\n",
        "  input_train = get_features(extractor,input_train);\n",
        "  input_validate = get_features(extractor,input_validate);\n",
        "  # load the classifier\n",
        "  binary_classifier = load_model(model_type,series,anomaly,classifier);\n",
        "  # obtaining the cassifications\n",
        "  input_train = binary_classifier.predict(input_train);\n",
        "  input_validate = binary_classifier.predict(input_validate);\n",
        "  return input_train,output_train,input_validate,output_validate;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEHXjJJcPfUX",
        "colab_type": "text"
      },
      "source": [
        "# **Functions for model evaluation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Fx0T_TPjMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(history):\n",
        "  # drawing the model graph\n",
        "  pd.DataFrame(history.history).plot(figsize=(10, 7));\n",
        "  plt.grid(True);\n",
        "  plt.gca().set_ylim(0, 1);\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHWzlCbpQZja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_extractor(model:Model,series,anomaly):\n",
        "  # getting test data\n",
        "  input_test,output_test = get_data_test(series,anomaly);\n",
        "  # processing it to test the extractor specifically: getting one image from each patient only\n",
        "  input_test = get_one_image(input_test);\n",
        "  return model.evaluate(input_test,output_test);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPBNP7JNmNd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_classifier(extractor:Model,classifier:Model,series,anomaly):\n",
        "  # getting test data\n",
        "  input_test,output_test = get_data_test(series,anomaly);\n",
        "  # processing it to test the extractor specifically: getting one image from each patient only\n",
        "  input_test = get_features(extractor,input_test);\n",
        "  return classifier.evaluate(input_test,output_test);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SASl0M4V-PXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_regressor(extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n",
        "  # getting the regressor\n",
        "  regressor = load_model(model_type,anomaly,regressor);\n",
        "  # getting the test classifications\n",
        "  pred_axial,output_test = get_classifications_test(extractor_axial,model_type,axial,anomaly);\n",
        "  pred_sagittal,output_test = get_classifications_test(extractor_sagittal,model_type,sagittal,anomaly);\n",
        "  pred_coronal,output_test = get_classifications_test(extractor_coronal,model_type,coronal,anomaly);\n",
        "  # combining the predictions\n",
        "  input_train = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n",
        "  return regressor.evaluate(input_test,output_test);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytG2jxdc_PMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classifications_test(extractor:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_test,output_test = get_data_test(series,anomaly,start = 0,batch_size = 5);\n",
        "  # obtaining the features\n",
        "  input_test = get_features(extractor,input_train);\n",
        "  # load the classifier\n",
        "  binary_classifier = load_model(model_type,series,anomaly,classifier);\n",
        "  # obtaining the cassifications\n",
        "  input_test = binary_classifier.predict(input_train);\n",
        "  return input_test,output_test;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHJEl2sR6oF",
        "colab_type": "text"
      },
      "source": [
        "# **Functions to manipualte saved models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLK3NnZ1R-eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_type,series,anomaly,model_part):\n",
        "  model_path = path_model + delim + model_type + delim + model_part + delim + series + '_' + anomaly + extension_model;\n",
        "  return tf.keras.models.load_model(model_path);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP43Ly7-8Dyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_regressor(model_type,anomaly,model_part):\n",
        "  model_path = path_model + delim + model_type + delim + model_part + delim + anomaly + extension_model;\n",
        "  return tf.keras.models.load_model(model_path);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dTwqXjQ_EuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}