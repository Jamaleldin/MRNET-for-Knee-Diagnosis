{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRNet_models_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1CfZr__riY1RwLWtPx4FhIhmCHGfX1rVn",
      "authorship_tag": "ABX9TyPlhT5PAO9IWs2zuW0gYqXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamaleldin/MRNET-for-Knee-Diagnosis/blob/master/MRNet_models_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwQJbS4DNac",
        "colab_type": "text"
      },
      "source": [
        "## **All needed library imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQvR4fkNDRyq",
        "colab_type": "code",
        "outputId": "cc6ebcbb-423d-4905-8d78-c9e6bbdfea91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqvYMolL1w_D",
        "colab_type": "text"
      },
      "source": [
        "# **All needed labels and constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnCBigQXyCjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we define the labels to be used in obtaining the data\n",
        "# series\n",
        "axial = 'axial';\n",
        "coronal = 'coronal';\n",
        "sagittal = 'sagittal';\n",
        "# data set\n",
        "valid = 'valid';\n",
        "train = 'train';\n",
        "# symptom\n",
        "acl = 'acl';\n",
        "meniscal = 'meniscus';\n",
        "abnormal = 'abnormal';\n",
        "# models\n",
        "vgg = 'VGG16';\n",
        "inception = 'Inception V3';\n",
        "resnet = 'Resnet';\n",
        "# model types\n",
        "extractor = 'Extractor';\n",
        "classifier = 'Classifier';\n",
        "regressor = 'Regressor';\n",
        "# paths\n",
        "path_data = '/content/drive/My Drive/MRNET data set/MRNet-v1.0';\n",
        "path_model = '/content/drive/My Drive/Models';\n",
        "delim = '/';\n",
        "# extensions\n",
        "extension_model = '.h5';\n",
        "extension_numpy = '.npy';\n",
        "extension_csv = '.csv';\n",
        "# total data\n",
        "total_train = 1130;\n",
        "total_test = 120;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ITqvN9Pxq1E",
        "colab_type": "text"
      },
      "source": [
        "# **Obtaining the data for training in a correct format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDisfeedicdn",
        "colab_type": "code",
        "outputId": "c43fa386-83ea-490f-c330-78f000e87102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# importing the drive to obtain the data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW7xpE0nFUH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipping the data zip file run once\n",
        "!unzip '/content/drive/My Drive/MRNET data set/MRNet-v1.0.zip' -d '/content/drive/My Drive/MRNET data set'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_sk_qq714vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cd68df1e-cc47-41f4-98d5-eab6ad1c6c47"
      },
      "source": [
        "# This function is used to obtain the data from drive to memory for training\n",
        "def get_data_train(series,anomaly,start = 0,batch_size = total_train):\n",
        "  input_data = [];\n",
        "  for exam_no in range(start,batch_size + start):\n",
        "      path_input = path_data + delim  + train + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n",
        "      input_data.append(normalize(np.stack([np.load(path_input)] * 3, axis = 3)))\n",
        "  path_output = path_data + delim + train + '-' + anomaly + extension_csv;\n",
        "  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n",
        "  return input_data, output_data[start:batch_size + start];"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8015cf461010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This function is used to obtain the data from drive to memory for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_data_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manomaly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mexam_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mpath_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelim\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexam_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'04d'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextension_numpy\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e5s8drLFYwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b9e6f594-db9e-4913-8e17-3e81b391c583"
      },
      "source": [
        "# This function is used to obtain the data from drive to memory for testing\n",
        "def get_data_test(series,anomaly,start = 0,batch_size = total_test):\n",
        "  input_data = [];\n",
        "  for iterator in range(start,batch_size + start):\n",
        "      exam_no = iterator + total_train;\n",
        "      path_input = path_data + delim  + valid + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n",
        "      input_data.append(normalize(np.stack([np.load(path_input)] * 3, axis = 3)))\n",
        "  path_output = path_data + delim + valid + '-' + anomaly + extension_csv;\n",
        "  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n",
        "  return input_data, output_data[start:batch_size + start];"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-76e65e0e342f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This function is used to obtain the data from drive to memory for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_data_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manomaly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mexam_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtotal_train\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByiaFQDs13YQ",
        "colab_type": "text"
      },
      "source": [
        "# **Data normalization part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJA7UyZs18MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalizing the image pixels to be a number from 0 to 1\n",
        "def normalize(X):\n",
        "  return X/255.;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17OHcLkqYLQY",
        "colab_type": "text"
      },
      "source": [
        "# **Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQU_i1J8YQcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on augmentation techniques defined in the paper\n",
        "def get_generators():\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rotation_range=25,\n",
        "      width_shift_range=[-25,25],\n",
        "      height_shift_range=[-25,25],\n",
        "      horizontal_flip=True);\n",
        "  validation_datagen = ImageDataGenerator();\n",
        "  return train_datagen,validation_datagen;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsEVz5V8CB-K",
        "colab_type": "text"
      },
      "source": [
        "# **Data splitting part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6mSwzflCHLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting the data to train and validate with ratios 90% to 10%\n",
        "def split(X , Y):\n",
        "  return X[:1017],Y[:1017],X[1017:],Y[1017:];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4uMa5xI90O",
        "colab_type": "text"
      },
      "source": [
        "# **Training functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67C5BfqJJCgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_extractor(model:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # processing to train for the extractor specifically: obtaining only one image from each patient\n",
        "  input_train = get_one_image(input_train);\n",
        "  input_validate = get_one_image(input_validate);\n",
        "  # performing data augmentation\n",
        "  train_datagen,validation_datagen = get_generators();\n",
        "  train_generator = train_datagen.flow(input_train,output_train, batch_size = 20);\n",
        "  validation_generator = validation_datagen.flow(input_validate,output_validate, batch_size = 20);\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + extractor + delim+series + '_' + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = model.fit_generator(train_generator, validation_data=validation_generator, epochs=50, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwhjnY5nN2hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_image(input_train):\n",
        "  new_input_train = [];\n",
        "  for curr in input_train:\n",
        "     new_input_train.append(curr[0]);\n",
        "  return np.array(new_input_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvHgIA-tjsSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(extractor:Model,binary_classifier:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # obtaining the data specifically for the classifier by getting the extracted features\n",
        "  input_train = get_features(extractor,input_train);\n",
        "  input_validate = get_features(extractor,input_validate);\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + classifier + delim + series + '_' + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = binary_classifier.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co61bPJokios",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(extractor:Model,input):\n",
        "  # forming the extraction model\n",
        "  model = keras.models.Sequential();\n",
        "  model.add(extractor);\n",
        "  model.add(keras.layers.GlobalAveragePooling2D(data_format='channels_last'));\n",
        "  # extracting the features\n",
        "  result = [];\n",
        "  for curr in input:\n",
        "    # getting the predictions\n",
        "    features_extracted = model.predict(curr);\n",
        "    # getting maximum between the batches\n",
        "    max_features = np.max(features_extracted,axis=0);  \n",
        "    result.append(max_features);\n",
        "  return np.array(result);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtlVeEI450Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_regressor(logistic_regressor:Model,extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n",
        "  # predictions of all classifiers:\n",
        "  pred_axial,output_train,val_axial,output_validate = get_classifications(extractor_axial,model_type,axial,anomaly);\n",
        "  pred_sagittal,output_train,val_sagittal,output_validate = get_classifications(extractor_sagittal,model_type,sagittal,anomaly);\n",
        "  pred_coronal,output_train,val_coronal,output_validate = get_classifications(extractor_coronal,model_type,coronal,anomaly);\n",
        "  # combining the predictions\n",
        "  input_train = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n",
        "  input_validate = np.array([val_axial,val_coronal,val_sagittal]).reshape(3,val_axial.shape[0]).T;\n",
        "  # defining the call backs for training\n",
        "  save_path = path_model + delim + model_type + delim + regressor + delim + anomaly + extension_model;\n",
        "  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n",
        "  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n",
        "  # performing a traininig operation with a batch size to overcome any overfitting\n",
        "  training_history = logistic_regressor.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n",
        "  # plotting the graph for the training history\n",
        "  plot(training_history);\n",
        "  return training_history;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvjxrjGg6FMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classifications(extractor:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_train,output_train = get_data_train(series,anomaly);\n",
        "  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n",
        "  # obtaining the features\n",
        "  input_train = get_features(extractor,input_train);\n",
        "  input_validate = get_features(extractor,input_validate);\n",
        "  # load the classifier\n",
        "  binary_classifier = load_model(model_type,series,anomaly,classifier);\n",
        "  # obtaining the cassifications\n",
        "  input_train = binary_classifier.predict(input_train);\n",
        "  input_validate = binary_classifier.predict(input_validate);\n",
        "  return input_train,output_train,input_validate,output_validate;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEHXjJJcPfUX",
        "colab_type": "text"
      },
      "source": [
        "# **Functions for model evaluation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Fx0T_TPjMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(history):\n",
        "  # drawing the model graph\n",
        "  pd.DataFrame(history.history).plot(figsize=(10, 7));\n",
        "  plt.grid(True);\n",
        "  plt.gca().set_ylim(0, 1);\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHWzlCbpQZja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_extractor(model:Model,series,anomaly):\n",
        "  # getting test data\n",
        "  input_test,output_test = get_data_test(series,anomaly);\n",
        "  # processing it to test the extractor specifically: getting one image from each patient only\n",
        "  input_test = get_one_image(input_test);\n",
        "  return model.evaluate(input_test,output_test);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPBNP7JNmNd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_classifier(extractor:Model,classifier:Model,series,anomaly):\n",
        "  # getting test data\n",
        "  input_test,output_test = get_data_test(series,anomaly);\n",
        "  # processing it to test the extractor specifically: getting one image from each patient only\n",
        "  input_test = get_features(extractor,input_test);\n",
        "  return classifier.evaluate(input_test,output_test);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SASl0M4V-PXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_regressor(extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n",
        "  # getting the regressor\n",
        "  logistic_regressor = load_model(model_type,anomaly,regressor);\n",
        "  # getting the test classifications\n",
        "  pred_axial,output_test = get_classifications_test(extractor_axial,model_type,axial,anomaly);\n",
        "  pred_sagittal,output_test = get_classifications_test(extractor_sagittal,model_type,sagittal,anomaly);\n",
        "  pred_coronal,output_test = get_classifications_test(extractor_coronal,model_type,coronal,anomaly);\n",
        "  # combining the predictions\n",
        "  input_test = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n",
        "  return logistic_regressor.evaluate(input_test,output_test);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytG2jxdc_PMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classifications_test(extractor:Model,model_type,series,anomaly):\n",
        "  # obtaining the data\n",
        "  input_test,output_test = get_data_test(series,anomaly);\n",
        "  # obtaining the features\n",
        "  input_test = get_features(extractor,input_train);\n",
        "  # load the classifier\n",
        "  binary_classifier = load_model(model_type,series,anomaly,classifier);\n",
        "  # obtaining the cassifications\n",
        "  input_test = binary_classifier.predict(input_train);\n",
        "  return input_test,output_test;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQZ9eIM6Yok",
        "colab_type": "text"
      },
      "source": [
        "# **Functions to get the model predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIeW8-i86es_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extractors is a List of the saved extractors containing: axial-abnormal,sagittal-abnormal,coronal-abnormal,axial-acl,\n",
        "# sagittal-acl coronal-acl,axial-meniscal,sagittal-meniscal,coronal-meniscal.\n",
        "def predict(extractors:list,input_axial,input_sagittal,input_coronal,model_type):\n",
        "  inputs = [input_axial,input_sagittal,input_coronal];\n",
        "  # getting the prediction for each anomaly\n",
        "  pred_abnormal = predict_anomaly(extractors[0:3],inputs,abnormal,model_type);\n",
        "  pred_acl = predict_anomaly(extractors[3:6],inputs,acl,model_type);\n",
        "  pred_meniscal = predict_anomaly(extractors[6:9],inputs,meniscal,model_type);\n",
        "  # combining the predictions and multiplying by 100\n",
        "  predictions = [pred_abnormal,pred_acl,pred_meniscal] * 100;\n",
        "  # printing the predictions\n",
        "  str_abnormal = abnormal + ' : ' +  str(predictions[0]) + '%';\n",
        "  str_acl = acl + ' : ' + str(predictions[1]) + '%';\n",
        "  str_meniscal = meniscal +' : ' + str(predictions[2]) + '%';\n",
        "  print(str_abnormal);\n",
        "  print(str_acl);\n",
        "  print(str_meniscal);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6CzIWZm7iLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_anomaly(extractors:list,inputs:list,anomaly,model_type):\n",
        "  # getting the classifiers\n",
        "  classifier_axial = load_model(model_type,axial,anomaly,classifier);\n",
        "  classifier_sagittal = load_model(model_type,sagittal,anomaly,classifier);\n",
        "  classifier_coronal = load_model(model_type,coronal,anomaly,classifier);\n",
        "  # getting the regressor\n",
        "  logistic_regressor = load_regressor(model_type,anomaly,regressor);\n",
        "  # getting classifications\n",
        "  pred_axial = get_classification_single(extractors[0],classifier_axial,inputs[0]);\n",
        "  pred_sagittal = get_classification_single(extractors[1],classifier_sagittal,inputs[1]);\n",
        "  pred_coronal = get_classification_single(extractors[2],classifier_coronal,inputs[2]);\n",
        "  # combining the predictions\n",
        "  input_regressor = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n",
        "  # final prediction is returned\n",
        "  return logistic_regressor.predict(input_regressor);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnyEpeQB-UoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classification_single(extractor:Model,binary_classifier:Model,input_single):\n",
        "  # obtaining the features\n",
        "  result = get_features(extractor,input_single);\n",
        "  # obtaining the cassifications\n",
        "  result = binary_classifier.predict(result);\n",
        "  return result;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHJEl2sR6oF",
        "colab_type": "text"
      },
      "source": [
        "# **Functions to manipualte saved models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLK3NnZ1R-eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_type,series,anomaly,model_part):\n",
        "  model_path = path_model + delim + model_type + delim + model_part + delim + series + '_' + anomaly + extension_model;\n",
        "  return tf.keras.models.load_model(model_path);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP43Ly7-8Dyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_regressor(model_type,anomaly,model_part):\n",
        "  model_path = path_model + delim + model_type + delim + model_part + delim + anomaly + extension_model;\n",
        "  return tf.keras.models.load_model(model_path);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbhpTRjG4Zoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}